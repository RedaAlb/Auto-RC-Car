{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "- Look into using `Dataset` object for the datasets [helpful link](https://www.tensorflow.org/guide/data).\n",
    "- Maybe use test set as well, rather than just testing it physically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\"forward\", \"left\", \"right\"]  # Also represents the class names.\n",
    "MODEL_I = 0  # Model index, determines which module to train, hence which data to use, (chosen from MODELS).\n",
    "\n",
    "DATA_PATH = f\"data/{MODELS[MODEL_I]}_model_data\"\n",
    "\n",
    "\n",
    "VALIDATION_SPLIT = 0.1\n",
    "IMGS_SHAPE = (240, 320, 3)\n",
    "BATCH_SIZE = 64  # Needs to be even, because half will be original data, half augmented data.\n",
    "SEED_NUM = 2\n",
    "\n",
    "\n",
    "# Data augmentation parameters\n",
    "ROT_RANGE = 10\n",
    "BRIGHT_MIN = 0.2\n",
    "BRIGHT_MAX = 1.5\n",
    "HORI_FLIP = True\n",
    "\n",
    "\n",
    "# General setup\n",
    "tf.random.set_seed(SEED_NUM)\n",
    "np.random.seed(SEED_NUM)\n",
    "random.seed(SEED_NUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data sequence class to setup the generators and manage custom batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSequence(tf.keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, validation_split, img_shape, batch_size):\n",
    "        self.validation_split = validation_split\n",
    "        self.img_shape = img_shape\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    \n",
    "    # Creating the training and validation generators\n",
    "    def create_train_val_gens(self, steering_frames, data_path, seed_num):\n",
    "        img_gen = tf.keras.preprocessing.image.ImageDataGenerator(validation_split=self.validation_split,\n",
    "                                                                  rescale=1./255)\n",
    "        \n",
    "        img_gen.fit(steering_frames)\n",
    "        \n",
    "        self.train_data_flow = img_gen.flow_from_directory(directory=data_path,\n",
    "                                                      target_size=(self.img_shape[0], self.img_shape[1]),\n",
    "                                                      batch_size=self.batch_size // 2, #  /2 because half will be augmented.\n",
    "                                                      seed=seed_num,\n",
    "                                                      subset=\"training\")\n",
    "\n",
    "\n",
    "        self.val_data_flow = img_gen.flow_from_directory(directory=data_path,\n",
    "                                                    target_size=(self.img_shape[0], self.img_shape[1]),\n",
    "                                                    batch_size=self.batch_size,\n",
    "                                                    seed=seed_num,\n",
    "                                                    subset=\"validation\")\n",
    "    \n",
    "    # Creating the generator for data augmentation.\n",
    "    def create_aug_gen(self, steering_frames, rot_range, bright_min, bright_max, hori_flip):\n",
    "        # Note that I am using the original data and data augmented data, because this works best for\n",
    "        # behavioral cloning tasks.\n",
    "        self.datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            rotation_range=rot_range,\n",
    "            brightness_range=(bright_min, bright_max),\n",
    "            horizontal_flip=hori_flip\n",
    "        )\n",
    "\n",
    "        self.datagen.fit(steering_frames)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.train_data_flow.samples // self.batch_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        batch_half1_org = self.train_data_flow.next()  # Getting batch_size/2 samples from original data.\n",
    "        \n",
    "        # Getting the other half from the augmentation generator.\n",
    "        datagen_flow = self.datagen.flow(batch_half1_org[0], batch_half1_org[1], batch_size=self.batch_size // 2)\n",
    "        batch_half2_aug = datagen_flow.next()\n",
    "        \n",
    "        x_batch = np.vstack((batch_half1_org[0], batch_half2_aug[0]))\n",
    "        y_batch = np.vstack((batch_half1_org[1], batch_half2_aug[1]))\n",
    "        \n",
    "        return (x_batch, y_batch)\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6477 images belonging to 3 classes.\n",
      "Found 718 images belonging to 3 classes.\n",
      "Temp samples: (32, 240, 320, 3) (32, 3)\n"
     ]
    }
   ],
   "source": [
    "data_seq = DataSequence(VALIDATION_SPLIT, IMGS_SHAPE, BATCH_SIZE)\n",
    "\n",
    "# This is loaded/needed so I can use the training data to fit the generators.\n",
    "with open(f\"data/steering_frames_{MODELS[MODEL_I]}.npy\", \"rb\") as file:\n",
    "    steering_frames = np.load(file)  # This will be deleted from memory later in this cell.\n",
    "\n",
    "data_seq.create_train_val_gens(steering_frames, DATA_PATH, SEED_NUM)\n",
    "data_seq.create_aug_gen(steering_frames, ROT_RANGE, BRIGHT_MIN, BRIGHT_MAX, HORI_FLIP)\n",
    "\n",
    "\n",
    "del steering_frames  # No longer needed, deleting from memory.\n",
    "\n",
    "\n",
    "# Just getting a small sample f the training data to visualise and test the data augmentation with.\n",
    "x_sample_batch, y_sample_batch = data_seq.train_data_flow.next()\n",
    "\n",
    "print(\"Temp samples:\", x_sample_batch.shape, y_sample_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot images in a grid\n",
    "def plot_imgs(images, labels, rows=3, cols=3, fig_w=15, fig_h=10):\n",
    "    fig, axis = plt.subplots(rows, cols, figsize=(fig_w, fig_h))\n",
    "    fig.tight_layout()\n",
    "\n",
    "    sample_index = 0\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            img = images[sample_index]\n",
    "            img = img * 255\n",
    "            img = img.astype(int)\n",
    "            \n",
    "            label = labels[sample_index]\n",
    "            \n",
    "            sample_index += 1\n",
    "\n",
    "            ax = axis[row, col]\n",
    "            ax.set_title(str(label))\n",
    "            ax.imshow(img)\n",
    "            ax.axis(\"off\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_imgs(x_sample_batch, y_sample_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_rot = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=ROT_RANGE,\n",
    ")\n",
    "\n",
    "data_flow = datagen_rot.flow(x_sample_batch, y_sample_batch, batch_size=32)\n",
    "plot_imgs(data_flow.next()[0], data_flow.next()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_bright = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    brightness_range=(BRIGHT_MIN, BRIGHT_MAX),\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "data_flow = datagen_bright.flow(x_sample_batch, y_sample_batch, batch_size=32)\n",
    "plot_imgs(data_flow.next()[0], data_flow.next()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Horizontal Flipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_flip = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    horizontal_flip=HORI_FLIP,\n",
    ")\n",
    "\n",
    "data_flow = datagen_flip.flow(x_sample_batch, y_sample_batch, batch_size=32)\n",
    "plot_imgs(data_flow.next()[0], data_flow.next()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing/visualising all data augmentations together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aug_data = data_seq.datagen.flow(x_sample_batch, y_sample_batch, batch_size=32).next()\n",
    "\n",
    "aug_batch_imgs = data_aug_data[0]\n",
    "aug_batch_labels = data_aug_data[1]\n",
    "\n",
    "print(\"Augmented batch:\", aug_batch_imgs.shape, aug_batch_labels.shape)\n",
    "\n",
    "plot_imgs(aug_batch_imgs, aug_batch_labels, rows=5, fig_h=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 240, 320, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 120, 160, 24)      1824      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 20, 26, 24)        14424     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 4, 36)          21636     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 432)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               43300     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 81,487\n",
      "Trainable params: 81,487\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Please note this is currently just a dummy model, just to test.\n",
    "\n",
    "inputs = tf.keras.Input(shape=IMGS_SHAPE)\n",
    "\n",
    "x = layers.Conv2D(24, (5,5), padding=\"same\", strides=2, activation=\"elu\")(inputs)  # org\n",
    "x = layers.Conv2D(24, (5,5), strides=6, activation=\"elu\")(x)\n",
    "x = layers.Conv2D(36, (5,5), strides=6, activation=\"elu\")(x)\n",
    "# x = layers.Conv2D(48, (5,5), strides=2, activation=\"elu\")(x)\n",
    "# x = layers.Conv2D(64, (3,3), activation=\"elu\")(x)\n",
    "# x = layers.Conv2D(64, (3,3), activation=\"elu\")(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "x = layers.Dense(100, activation=\"elu\")(x)\n",
    "\n",
    "# x = layers.Dense(50, activation=\"elu\")(x)\n",
    "# x = layers.Dense(10, activation=\"elu\")(x)\n",
    "outputs = layers.Dense(3, activation=\"softmax\")(x)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-3)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "\n",
    "history = model.fit(data_seq,\n",
    "                    validation_data = data_seq.val_data_flow,\n",
    "                    validation_steps = data_seq.val_data_flow.samples // BATCH_SIZE,\n",
    "                    epochs = EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For custom epoch step:\n",
    "\n",
    "# for e in range(EPOCHS):\n",
    "#     print(\"----------------------------------------Epoch\", e, \":\")\n",
    "    \n",
    "#     batches = 0\n",
    "    \n",
    "#     for x_batch, y_batch in data_seq.train_data_flow:\n",
    "#         model.fit(x=x_batch, y=y_batch,\n",
    "#                   steps_per_epoch = 1,\n",
    "#                   batch_size = x_batch.shape[0],\n",
    "#                   validation_data = data_seq.val_data_flow,\n",
    "#                   validation_steps = 1,\n",
    "#                   verbose=2)\n",
    "        \n",
    "#         batches += 1\n",
    "        \n",
    "#         if batches >= data_seq.train_data_flow.samples // BATCH_SIZE:\n",
    "#             break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_gpu] *",
   "language": "python",
   "name": "conda-env-tf_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 353,
   "position": {
    "height": "695px",
    "left": "369.972px",
    "right": "20px",
    "top": "265.938px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
