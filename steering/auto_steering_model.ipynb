{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Look into using the `Dataset` object for the datasets [helpful link](https://www.tensorflow.org/guide/data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data_sequence import DataSequence\n",
    "\n",
    "\n",
    "MODELS = [\"forward\", \"left\", \"right\"]  # Also represents the class names.\n",
    "MODEL_I = 0  # Model index, determines which model to train, hence which data to use, (chosen from MODELS).\n",
    "\n",
    "\n",
    "# The folder where the data is stored. This is here to easily switch between local and Google Colab env.\n",
    "PATH_TO_DATA_DIR = \"data\"\n",
    "# PATH_TO_DATA_DIR = \"drive/My Drive/Auto_RC_Car/data\"  # For when using Google Colab.\n",
    "\n",
    "\n",
    "VALIDATION_SPLIT = 0.2\n",
    "TEST_SPLIT = 0  # Zero for no test set. Note: Only applicable when custom batches are used.\n",
    "# For testing, I simply test it on the road physically for a more accurate test.\n",
    "\n",
    "\n",
    "USE_DATA_AUG = False\n",
    "BATCH_SIZE = 256  # If using custom batches, needs to be even because half will be original data, half augmented data.\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "REMOVE_F_SAMPLES = True  # Whether to remove forward samples or not (to balance the data).\n",
    "F_SAMPLES_TO_REMOVE = 4000\n",
    "\n",
    "\n",
    "USE_ROAD_MASK = True  # Whether to excract the road mask for each frame and use as an additional input.\n",
    "USE_ROAD_CDIST = True  # Whether to excract the central road distance (mid bottom->mid top) to use as an additional input.\n",
    "\n",
    "\n",
    "CROP_HEIGHT = True  # Whether to crop the image from top to HEIGHT_CROP pixels.\n",
    "HEIGHT_TO_CROP = 95  # How many pixels to crop from the top.\n",
    "RESIZE_IMG = True  # Whether to resize the images or not to the value below, IMG_SHAPE.\n",
    "IMG_SHAPE = (200, 100)  # (w, h)\n",
    "USE_YUV = True  # Whether to use YUV colour space, otherwise, RGB.\n",
    "BLUR_IMG = True  # Whether to blur the images as part of the pre-processing.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "USE_CUSTOM_BATCHES = False  # Whether to use batches that have the same ratio of class samples.\n",
    "# Ratios of classes in each batch. This can be done because there are significantly more forward samples (staying in lane). However, I am still\n",
    "# experimenting with using custom batches.\n",
    "FORWARD_RATIO = 0.5\n",
    "LEFT_RATIO    = 0.25\n",
    "RIGHT_RATIO   = 0.25\n",
    "\n",
    "\n",
    "\n",
    "# Data augmentation parameters\n",
    "ROT_RANGE = 4\n",
    "BRIGHT_MIN = 0.6\n",
    "BRIGHT_MAX = 1.32\n",
    "HORI_FLIP = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SEED_NUM = 6\n",
    "tf.random.set_seed(SEED_NUM)\n",
    "np.random.seed(SEED_NUM)\n",
    "random.seed(SEED_NUM)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_seq = DataSequence(BATCH_SIZE, NUM_CLASSES, USE_DATA_AUG, SEED_NUM, USE_CUSTOM_BATCHES)\n",
    "\n",
    "data_seq.load_data(path_to_data_dir=PATH_TO_DATA_DIR, model_to_load=MODELS[MODEL_I])\n",
    "data_seq.pre_process_data(CROP_HEIGHT, HEIGHT_TO_CROP, RESIZE_IMG, IMG_SHAPE, USE_YUV, BLUR_IMG)\n",
    "data_seq.split_into_classes(REMOVE_F_SAMPLES, F_SAMPLES_TO_REMOVE)\n",
    "data_seq.split_val_test(VALIDATION_SPLIT, TEST_SPLIT, FORWARD_RATIO, LEFT_RATIO, RIGHT_RATIO)\n",
    "\n",
    "\n",
    "if USE_DATA_AUG:\n",
    "    data_seq.create_aug_gen(ROT_RANGE, BRIGHT_MIN, BRIGHT_MAX, HORI_FLIP)\n",
    "    \n",
    "if USE_ROAD_MASK:\n",
    "    data_seq.excract_road_masks()\n",
    "    \n",
    "if USE_ROAD_CDIST:\n",
    "    data_seq.excract_road_masks()  # The masks are still needed to get the distances.\n",
    "    data_seq.excract_centre_distance()\n",
    "\n",
    "\n",
    "\n",
    "# Just getting a small sample of the training data to visualise.\n",
    "x_sample_batch, y_sample_batch = data_seq.get_batch(32)\n",
    "print(\"\\nTemp samples:\", x_sample_batch.shape, y_sample_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For plotting images in a grid.\n",
    "def plot_imgs(images, labels, rows=3, cols=3, fig_w=15, fig_h=10):\n",
    "    fig, axis = plt.subplots(rows, cols, figsize=(fig_w, fig_h))\n",
    "    fig.tight_layout()\n",
    "\n",
    "    sample_index = 0\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            if sample_index >= images.shape[0]:\n",
    "                break\n",
    "            img = images[sample_index]            \n",
    "            label = labels[sample_index]\n",
    "            \n",
    "            sample_index += 1\n",
    "\n",
    "            ax = axis[row, col]\n",
    "            ax.set_title(str(label))\n",
    "            ax.imshow(img)\n",
    "            #ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_imgs(x_sample_batch, y_sample_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Samples per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: when using custom batches, this does not matter because each batch will have equal ratio of samples as set above.\n",
    "if not USE_CUSTOM_BATCHES:  # If using custom batches, the number of class samples per batch will have a fixed ratio.\n",
    "    a_dictionary = {\"Forward\": data_seq.forward_x.shape[0], \"Left\": data_seq.left_x.shape[0], \"Right\": data_seq.right_x.shape[0]}\n",
    "    keys = a_dictionary.keys()\n",
    "    values = a_dictionary.values()\n",
    "\n",
    "    plt.bar(keys, values) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing data augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_DATA_AUG:\n",
    "    data_flow = data_seq.datagen.flow(x_sample_batch, y_sample_batch, batch_size=32)\n",
    "    data_flow_batch = data_flow.next()\n",
    "    plot_imgs(data_flow_batch[0], data_flow_batch[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising road masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_imgs(data_seq.x_train_masks, data_seq.y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising road centre distances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_imgs_idx = np.random.choice(data_seq.x_train.shape[0], 32)\n",
    "sample_imgs = data_seq.x_train[sample_imgs_idx]\n",
    "sample_imgs_dist = data_seq.x_train_dist[sample_imgs_idx] * IMG_SHAPE[1]\n",
    "\n",
    "for i in range(32):\n",
    "    # For drawing a line to represent the distance.\n",
    "    point1 = (IMG_SHAPE[0]//2, IMG_SHAPE[1])\n",
    "    point2 = (IMG_SHAPE[0]//2, int(IMG_SHAPE[1] - sample_imgs_dist[i]))\n",
    "\n",
    "    sample_imgs[i] = cv2.line(sample_imgs[i], point1, point2, (0, 255, 0), thickness=2)\n",
    "    \n",
    "\n",
    "plot_imgs(sample_imgs, sample_imgs_dist)  # Passing in sample_imgs_dist as the \"labels\" so distance is displayed on title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First model for the frames.\n",
    "input1_frames = tf.keras.Input(shape=(IMG_SHAPE[1], IMG_SHAPE[0], 3))\n",
    "\n",
    "in1_x = layers.Conv2D(24, 5, padding=\"same\", strides=2, activation=\"elu\")(input1_frames)\n",
    "in1_x = layers.Conv2D(36, 5, strides=2, activation=\"elu\")(in1_x)\n",
    "\n",
    "in1_x = layers.Conv2D(48, 5, padding=\"same\", strides=2, activation=\"elu\")(in1_x)\n",
    "in1_x = layers.Conv2D(64, 3, activation=\"elu\")(in1_x)\n",
    "\n",
    "in1_x = layers.Conv2D(64, 3, padding=\"same\", activation=\"elu\")(in1_x)\n",
    "in1_x = layers.MaxPooling2D(2)(in1_x)\n",
    "\n",
    "in1_x = layers.Dropout(0.5)(in1_x)\n",
    "\n",
    "\n",
    "fcl = layers.Flatten()(in1_x)  # Fully connected layer(s) (fcl)\n",
    "\n",
    "\n",
    "# Second model for the road masks.\n",
    "if USE_ROAD_MASK:\n",
    "    input2_masks = tf.keras.Input(shape=(IMG_SHAPE[1], IMG_SHAPE[0], 1))\n",
    "    in2_x = layers.Conv2D(16, 5, strides=2, activation=\"elu\")(input2_masks)\n",
    "    in2_x = layers.MaxPooling2D(4, padding=\"same\", strides=2)(in2_x)\n",
    "    in2_x = layers.Conv2D(28, 3, padding=\"same\", strides=2, activation=\"elu\")(in2_x)\n",
    "    in2_x = layers.MaxPooling2D(3, padding=\"same\", strides=1)(in2_x)\n",
    "    in2_x = layers.Conv2D(48, 3, strides=1, activation=\"elu\")(in2_x)\n",
    "    in2_x = layers.MaxPooling2D(2)(in2_x)\n",
    "    in2_x = layers.Flatten()(in2_x)\n",
    "\n",
    "    fcl = layers.concatenate([fcl, in2_x])\n",
    "\n",
    "    \n",
    "fcl = layers.Dense(100, activation=\"elu\")(fcl)\n",
    "fcl = layers.Dropout(0.5)(fcl)\n",
    "\n",
    "fcl = layers.Dense(50, activation=\"elu\")(fcl)\n",
    "fcl = layers.Dropout(0.5)(fcl)\n",
    "\n",
    "fcl = layers.Dense(10, activation=\"elu\")(fcl)\n",
    "\n",
    "# For the road centre distances.\n",
    "if USE_ROAD_CDIST:\n",
    "    input3_distances = tf.keras.Input(shape=(1, ))\n",
    "    fcl = layers.concatenate([fcl, input3_distances])\n",
    "\n",
    "fcl = layers.Dropout(0.5)(fcl)\n",
    "\n",
    "\n",
    "outputs = layers.Dense(3, activation=\"softmax\")(fcl)\n",
    "\n",
    "\n",
    "if USE_ROAD_MASK and USE_ROAD_CDIST:\n",
    "    model = tf.keras.Model(inputs=[input1_frames, input2_masks, input3_distances], outputs=outputs)\n",
    "elif USE_ROAD_MASK and not USE_ROAD_CDIST:\n",
    "    model = tf.keras.Model(inputs=[input1_frames, input2_masks], outputs=outputs)\n",
    "elif USE_ROAD_CDIST and not USE_ROAD_MASK:\n",
    "    model = tf.keras.Model(inputs=[input1_frames, input3_distances], outputs=outputs)\n",
    "else:\n",
    "    model = tf.keras.Model(inputs=input1_frames, outputs=outputs)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.000015)\n",
    "\n",
    "# Maybe loss needs  to be categorical_crossentropy.\n",
    "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch <= 1000:\n",
    "        return 0.001\n",
    "    elif epoch <= 1600:\n",
    "        return 0.0001\n",
    "    elif epoch <= 2200:\n",
    "        return 0.00005\n",
    "    else:\n",
    "        return 0.00003\n",
    "    \n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "#     tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Calculating the class weights if class weights were to be used.\n",
    "y_labels = []\n",
    "for label in data_seq.y_train:\n",
    "    if (label==np.array([1, 0, 0])).all(): y_labels.append(0)\n",
    "    elif (label==np.array([0, 1, 0])).all(): y_labels.append(1)\n",
    "    elif (label==np.array([0, 0, 1])).all(): y_labels.append(2)\n",
    "\n",
    "y_labels = np.array(y_labels)\n",
    "print(y_labels.shape)\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight(\"balanced\", np.unique(y_labels), y_labels)\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(\"Class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "MODEL_NAME = \"8_2_1_combined_noDAug_lowLR_fromStart\"  # The name the model will be saved in.\n",
    "SAVE_MODEL = True\n",
    "\n",
    "\n",
    "if not USE_CUSTOM_BATCHES:\n",
    "    \n",
    "    if USE_DATA_AUG:\n",
    "        history = model.fit(data_seq.datagen.flow(data_seq.x_train, data_seq.y_train, batch_size=BATCH_SIZE),\n",
    "                            steps_per_epoch=data_seq.x_train.shape[0]//BATCH_SIZE,\n",
    "                            validation_data=(data_seq.x_val, data_seq.y_val),\n",
    "                            epochs=EPOCHS,\n",
    "                            callbacks=callbacks)\n",
    "    else:\n",
    "        if not USE_ROAD_MASK and not USE_ROAD_CDIST:\n",
    "            history = model.fit(data_seq.x_train, data_seq.y_train, batch_size=BATCH_SIZE,\n",
    "                                steps_per_epoch=data_seq.x_train.shape[0]//BATCH_SIZE,\n",
    "                                validation_data=(data_seq.x_val, data_seq.y_val),\n",
    "                                epochs=EPOCHS,\n",
    "                                callbacks=callbacks)\n",
    "            \n",
    "        elif USE_ROAD_MASK and not USE_ROAD_CDIST:\n",
    "            history = model.fit([data_seq.x_train, data_seq.x_train_masks], data_seq.y_train, batch_size=BATCH_SIZE,\n",
    "                                steps_per_epoch=data_seq.x_train.shape[0]//BATCH_SIZE,\n",
    "                                validation_data=([data_seq.x_val, data_seq.x_val_masks], data_seq.y_val),\n",
    "                                epochs=EPOCHS,\n",
    "                                callbacks=callbacks)\n",
    "            \n",
    "        elif USE_ROAD_CDIST and not USE_ROAD_MASK:\n",
    "            history = model.fit([data_seq.x_train, data_seq.x_train_dist], data_seq.y_train, batch_size=BATCH_SIZE,\n",
    "                                steps_per_epoch=data_seq.x_train.shape[0]//BATCH_SIZE,\n",
    "                                validation_data=([data_seq.x_val, data_seq.x_val_dist], data_seq.y_val),\n",
    "                                epochs=EPOCHS,\n",
    "                                callbacks=callbacks)\n",
    "            \n",
    "        elif USE_ROAD_MASK and USE_ROAD_CDIST:\n",
    "            history = model.fit([data_seq.x_train, data_seq.x_train_masks, data_seq.x_train_dist],\n",
    "                                data_seq.y_train, batch_size=BATCH_SIZE,\n",
    "                                steps_per_epoch=data_seq.x_train.shape[0]//BATCH_SIZE,\n",
    "                                validation_data=([data_seq.x_val, data_seq.x_val_masks, data_seq.x_val_dist], data_seq.y_val),\n",
    "                                epochs=EPOCHS,\n",
    "                                callbacks=callbacks)\n",
    "\n",
    "    \n",
    "else:\n",
    "    history = model.fit(data_seq,\n",
    "                        steps_per_epoch = data_seq.train_n_samples // BATCH_SIZE,\n",
    "                        validation_data = (data_seq.x_val, data_seq.y_val),\n",
    "                        validation_steps = data_seq.x_val.shape[0] // BATCH_SIZE,\n",
    "                        epochs = EPOCHS,\n",
    "                        callbacks=callbacks,\n",
    "                        shuffle=False)\n",
    "\n",
    "# Saving the trained model and its history.\n",
    "if SAVE_MODEL:\n",
    "    model.save(f\"saved_models/model_{MODELS[MODEL_I][:1]}_{MODEL_NAME}.h5\")\n",
    "    \n",
    "    history_array = np.array([history.history[\"loss\"],\n",
    "                              history.history[\"accuracy\"],\n",
    "                              history.history[\"val_loss\"],\n",
    "                              history.history[\"val_accuracy\"]])\n",
    "    with open(f\"saved_models/history_{MODELS[MODEL_I][:1]}_{MODEL_NAME}.npy\", \"wb\") as file:\n",
    "        np.save(file, history_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.legend([\"training\", \"validation\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss plot\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history[\"val_accuracy\"])\n",
    "plt.legend([\"training\", \"validation\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing individual data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_rot = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    fill_mode=\"constant\",\n",
    "    rotation_range=ROT_RANGE,\n",
    ")\n",
    "\n",
    "data_flow = datagen_rot.flow(x_sample_batch, y_sample_batch, batch_size=32)\n",
    "plot_imgs(data_flow.next()[0], data_flow.next()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_bright = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    brightness_range=(BRIGHT_MIN, BRIGHT_MAX),\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "data_flow = datagen_bright.flow(x_sample_batch, y_sample_batch, batch_size=32)\n",
    "plot_imgs(data_flow.next()[0], data_flow.next()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Horizontal Flipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_flip = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    horizontal_flip=HORI_FLIP,\n",
    ")\n",
    "\n",
    "data_flow = datagen_flip.flow(x_sample_batch, y_sample_batch, batch_size=32)\n",
    "plot_imgs(data_flow.next()[0], data_flow.next()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_zoom = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    fill_mode=\"constant\",\n",
    "    zoom_range=[0.5, 1]\n",
    ")\n",
    "\n",
    "data_flow = datagen_zoom.flow(x_sample_batch, y_sample_batch, batch_size=32)\n",
    "plot_imgs(data_flow.next()[0], data_flow.next()[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_gpu] *",
   "language": "python",
   "name": "conda-env-tf_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 353,
   "position": {
    "height": "695px",
    "left": "227.948px",
    "right": "20px",
    "top": "304.91px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
